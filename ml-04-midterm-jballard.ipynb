{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Machine Learning Midterm Project: _Classification Analysis_\n",
    "\n",
    "## Jason Ballard\n",
    "\n",
    "#### Basehor, Kansas (CDT)\n",
    "\n",
    "#### April 6, 2025\n",
    "\n",
    "> Submission: GitHub Repository with Jupyter Notebook and Peer Review\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Œ Project Overview\n",
    "\n",
    "Organizations frequently need to classify data to support decision-making. \n",
    "For example, a healthcare provider may want to predict whether a patient has a specific condition based on lab results,\n",
    "or a business may classify customer behavior to tailor marketing strategies.\n",
    "Machine learning classification models help automate these decisions by recognizing patterns in historical data.\n",
    "\n",
    "This project demonstrates the ability to apply classification modeling techniques to a real-world dataset. You will:\n",
    "\n",
    "- Load and explore a dataset.\n",
    "- Analyze feature distributions and consider feature selection.\n",
    "- Train and evaluate a classification model.\n",
    "- Compare different classification approaches.\n",
    "- Document your work in a structured Jupyter Notebook.\n",
    "- Conduct a peer review of a classmateâ€™s project.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Utility\n",
    "import tabulate\n",
    "\n",
    "# Scikit-learn: Model Selection\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "\n",
    "# Scikit-learn: Models\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Scikit-learn: Preprocessing & Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Scikit-learn: Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1. Import and Inspect the Data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 1.1 Load the dataset and display the first 10 rows.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Mushroom Classification dataset\n",
    "column_names = [\n",
    "    \"class\", \"cap-shape\", \"cap-surface\", \"cap-color\", \"bruises\", \"odor\",\n",
    "    \"gill-attachment\", \"gill-spacing\", \"gill-size\", \"gill-color\",\n",
    "    \"stalk-shape\", \"stalk-root\", \"stalk-surface-above-ring\", \"stalk-surface-below-ring\",\n",
    "    \"stalk-color-above-ring\", \"stalk-color-below-ring\", \"veil-type\", \"veil-color\",\n",
    "    \"ring-number\", \"ring-type\", \"spore-print-color\", \"population\", \"habitat\"\n",
    "]\n",
    "\n",
    "data_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\"\n",
    "\n",
    "df = pd.read_csv(data_url, header=None, names=column_names)\n",
    "\n",
    "# Display first few rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 1.2 Check for missing values and display summary statistics.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"ðŸ” Missing Values:\\n\", missing_values[missing_values > 0])\n",
    "\n",
    "# Basic summary statistics (only works meaningfully after encoding)\n",
    "print(\"\\nðŸ“Š Summary Statistics (non-numeric preview):\")\n",
    "print(df.describe(include='all').T)\n",
    "\n",
    "# Check for class balance\n",
    "print(\"\\nðŸŽ¯ Class Distribution:\")\n",
    "print(df['class'].value_counts())\n",
    "\n",
    "# Visualize missing data if any\n",
    "if missing_values.any():\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(df.isnull(), cbar=False, cmap=\"YlGnBu\")\n",
    "    plt.title(\"Missing Data Heatmap\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2. Data Exploration and Preparation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 2.1 Explore data patterns and distributions\n",
    "  - Create histograms, boxplots, and count plots for categorical variables (as applicable).\n",
    "  - Identify patterns, outliers, and anomalies in feature distributions.\n",
    "  - Check for class imbalance in the target variable (as applicable).\n",
    "  ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class imbalance check\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='class', data=df, palette='Set2')\n",
    "plt.title(\"Class Distribution: Edible vs Poisonous\")\n",
    "plt.xlabel(\"Class (e = Edible, p = Poisonous)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Explore distributions of selected categorical features\n",
    "categorical_features = ['cap-shape', 'cap-surface', 'cap-color', 'odor', 'gill-color', 'habitat']\n",
    "\n",
    "# Count plots for each selected feature\n",
    "for feature in categorical_features:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.countplot(x=feature, data=df, palette='viridis', order=df[feature].value_counts().index)\n",
    "    plt.title(f\"Distribution of {feature}\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Check for rare categories or anomalies\n",
    "print(\"\\nðŸ”Ž Unique values per feature:\")\n",
    "for col in df.columns:\n",
    "    print(f\"{col:25} : {df[col].nunique()} unique values\")\n",
    "\n",
    "print(\"\\n Checking for rare categories (less than 10 occurrences):\")\n",
    "for col in df.columns:\n",
    "    counts = df[col].value_counts()\n",
    "    rare = counts[counts < 10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 2.2 Handle missing values and clean data\n",
    "  - Impute or drop missing values (as applicable).\n",
    "  - Remove or transform outliers (as applicable).\n",
    "  - Convert categorical data to numerical format using encoding (as applicable).\n",
    "  ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-check missing values before handling\n",
    "print(\"ðŸ” Missing Values Before Cleaning:\")\n",
    "print(df.isnull().sum()[df.isnull().sum() > 0])\n",
    "\n",
    "# Impute with most frequent value\n",
    "most_freq = df['stalk-root'].mode()[0]\n",
    "df['stalk-root'].replace(np.nan, most_freq, inplace=True)\n",
    "\n",
    "# After imputation, rename to df_clean\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Confirm cleaning\n",
    "print(\"\\nâœ… Missing Values After Cleaning:\")\n",
    "print(df_clean.isnull().sum().sum(), \"missing values remaining\")\n",
    "\n",
    "# Outlier handling â€“ for categorical data, focus on rare categories\n",
    "# Drop rare categories or group them into 'Other'\n",
    "\n",
    "# Encode categorical variables using Label Encoding\n",
    "# This works well with tree-based models and is simple\n",
    "\n",
    "df_encoded = df_clean.copy()\n",
    "label_encoders = {}\n",
    "\n",
    "for col in df_encoded.columns:\n",
    "    le = LabelEncoder()\n",
    "    df_encoded[col] = le.fit_transform(df_encoded[col])\n",
    "    label_encoders[col] = le  # Save for inverse_transform or decoding later\n",
    "\n",
    "# Confirm encoding\n",
    "print(\"\\nðŸ”¢ Encoded Data Sample:\")\n",
    "print(df_encoded.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 2.3 Feature selection and engineering\n",
    "  - Create new features (as applicable).\n",
    "  - Transform or combine existing features to improve model performance (as applicable).\n",
    "  - Scale or normalize data (as applicable).\n",
    "  ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df_encoded.drop(\"class\", axis=1)\n",
    "y = df_encoded[\"class\"]\n",
    "\n",
    "# Optional: Create interaction features (if any domain knowledge suggests it)\n",
    "# Example (not meaningful here without context):\n",
    "# X['odor_gill'] = X['odor'] * X['gill-color']  # Not used here, but shown for structure\n",
    "\n",
    "# Explore feature importance using a simple Decision Tree\n",
    "# This is a quick way to see which features are most important\n",
    "# and can help in feature selection\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "importances = pd.Series(dt.feature_importances_, index=X.columns)\n",
    "importances = importances.sort_values(ascending=False)\n",
    "\n",
    "# Plot top features\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=importances.values[:10], y=importances.index[:10])\n",
    "plt.title(\"Top 10 Important Features (Decision Tree)\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()\n",
    "\n",
    "# Scaling â€“ Required only for distance-based models (e.g., SVM, MLP)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Wrap into DataFrame (optional)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Confirm scaled data\n",
    "print(\"\\n Scaled Feature Sample:\")\n",
    "print(X_scaled_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3. Feature Selection and Justification\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3.1 Choose features and target\n",
    "  - Select two or more input features (numerical for regression, numerical and/or categorical for classification)\n",
    "  - Select a target variable (as applicable)\n",
    "    - Regression: Continuous target variable (e.g., price, temperature).\n",
    "    - Classification: Categorical target variable (e.g., gender, species).\n",
    "    - Clustering: No target variable.\n",
    "  - Justify your selection with reasoning.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (y)\n",
    "# Target variable: 'class' â€“ edible (e) or poisonous (p)\n",
    "# Features selected based on earlier Decision Tree feature importance\n",
    "\n",
    "selected_features = [\n",
    "    'odor',            # Most influential â€” strong indicator of toxicity\n",
    "    'gill-color',      # Indicates maturity and species â€” useful class separation\n",
    "    'spore-print-color', # Important taxonomic trait among mushrooms\n",
    "    'habitat',         # Environmental context â€” some mushrooms grow in specific regions\n",
    "    'population'       # Can hint at species commonality\n",
    "]\n",
    "\n",
    "# Feature and target definition\n",
    "X_selected = df_encoded[selected_features]\n",
    "y = df_encoded['class']  # 0 = edible, 1 = poisonous (after LabelEncoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3.2 Define X and y\n",
    "  - Assign input features to X\n",
    "  - Assign target variable to y (as applicable)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 â€“ Define X and y\n",
    "\n",
    "# Input features (selected from 3.1)\n",
    "X = df_encoded[[\n",
    "    'odor',\n",
    "    'gill-color',\n",
    "    'spore-print-color',\n",
    "    'habitat',\n",
    "    'population'\n",
    "]]\n",
    "\n",
    "# Target variable: 'class' (edible vs. poisonous)\n",
    "y = df_encoded['class']\n",
    "\n",
    "# Confirm shapes and sample\n",
    "print(f\" Features shape: {X.shape}\")\n",
    "print(f\" Target shape: {y.shape}\")\n",
    "print(\"\\n Feature Sample:\")\n",
    "print(X.head())\n",
    "\n",
    "print(\"\\n Target Sample:\")\n",
    "print(y.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4. Train a Model (Classification: Choose 1: Decision Tree, Random Forest, Logistic Regression)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 4.1 Split the data into training and test sets using train_test_split (or StratifiedShuffleSplit if class imbalance is an issue).\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Stratified Shuffle Split\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform the split\n",
    "for train_index, test_index in split.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "# Confirm distribution in each split\n",
    "print(\" Class distribution in full dataset:\")\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\n Class distribution in training set:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\n Class distribution in test set:\")\n",
    "print(y_test.value_counts(normalize=True))\n",
    "\n",
    "# Confirm shapes\n",
    "print(f\"\\n Training set: {X_train.shape}, Test set: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 4.2 Train model using Scikit-Learn model.fit() method.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Decision Tree model\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the model using training data\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Confirm training complete\n",
    "print(\"âœ… Decision Tree model trained successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Confirm training\n",
    "print(\"âœ… Random Forest model trained successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 4.3 Evalulate performance, for example:\n",
    "  - Regression: R^2, MAE, RMSE (RMSE has been recently updated)\n",
    "  - Classification: Accuracy, Precision, Recall, F1-score, Confusion Matrix\n",
    "  - Clustering: Inertia, Silhouette Score\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5. Improve the Model or Try Alternatives (Implement a Second Option)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 5.1 Train an alternative classifier (e.g., Decision Tree, Random Forest, Logistic Regression) OR adjust hyperparameters on the original model.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 5.2 Compare the performance of all models across the same performance metrics.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6. Final Thoughts & Insights\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 6.1 Summarize findings.\n",
    "---\n",
    "(write here)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 6.2 Discuss challenges faced.\n",
    "---\n",
    "(write here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 6.3 If you had more time, what would you try next?\n",
    "---\n",
    "(write here)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
